# 疑难杂症
## 1. task java.lang.UnsupportedOperationException: Numeric columns do not support multivalue rows.
- 原因:
  - 在摄入期间发现数据包含多个值的行，而indexer不知道如何处理.
  - 一般出现在CSV/TSV数据中
- 解决方案:
  - [为 multi-value 指定分隔符: listDelimiter](https://support.imply.io/hc/en-us/articles/360004103373-Data-ingestion-failure-due-to-error-Numeric-columns-do-not-support-multivalue-rows-)
  - [官网地址](https://druid.apache.org/docs/latest/ingestion/data-formats.html#csv)
  - ```json
     "parseSpec" : {
               "format" : "csv",
               "hasHeaderRow": true,
               "listDelimiter": "abc",
               "timestampSpec": {
                 "column": "TimeStamp"
               },
    ```
## 2. Ingestion was throttled.. because persists were pending
- 原因:
  - 发生在, IO写等待
  - index worker 工作原理是, 在内存中生成一个大小为maxRowInMemory的index, 然后在**到达maxRowInMemory阈值时, 将内存中的index split to disk**, 释放内存; 如果配置错误, 导致频繁的的split数据到disk, 由于磁盘IO有限, 就会产生上述错误.
- 解决[方案](https://support.imply.io/hc/en-us/articles/360009051253-Issue-Indexing-tasks-from-Kafka-or-Kinesis-are-finishing-successfully-but-without-any-data-ingested-):
  1. Increase taskCount
     - 数据量大的情况下, 增大task数, 分散IO压力, task数<= kafka partition数
  2. Increase maxRowsInMemory
     - 增大内存 buffer, 减少 split disk 次数
  3. Use a coarser segmentGranularity (e.g. change from HOUR to DAY)
     - 增大 segmentGranularity 可以减少 persists数量, 特别是stream 中 data 没有 time-ordered时.
## 3. Issue: Indexing tasks from Kafka or Kinesis are finishing successfully, but without any data ingested.
- [原文链接](https://support.imply.io/hc/en-us/articles/360009051253-Issue-Indexing-tasks-from-Kafka-or-Kinesis-are-finishing-successfully-but-without-any-data-ingested-)