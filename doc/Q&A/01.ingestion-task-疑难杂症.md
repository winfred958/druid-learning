# ingestion task 疑难杂症
## 1. task java.lang.UnsupportedOperationException: Numeric columns do not support multivalue rows.
- 原因:
  - 在摄入期间发现数据包含多个值的行，而indexer不知道如何处理.
  - 一般出现在CSV/TSV数据中
- 解决方案:
  - [为 multi-value 指定分隔符: listDelimiter](https://support.imply.io/hc/en-us/articles/360004103373-Data-ingestion-failure-due-to-error-Numeric-columns-do-not-support-multivalue-rows-)
  - [官网地址](https://druid.apache.org/docs/latest/ingestion/data-formats.html#csv)
  - ```json
     "parseSpec" : {
               "format" : "csv",
               "hasHeaderRow": true,
               "listDelimiter": "abc",
               "timestampSpec": {
                 "column": "TimeStamp"
               },
    ```
## 2. Ingestion was throttled.. because persists were pending
- 原因:
  - 发生在, IO写等待
  - index worker 工作原理是, 在内存中生成一个大小为maxRowInMemory的index, 然后在**到达maxRowInMemory阈值时, 将内存中的index split to disk**, 释放内存; 如果配置错误, 导致频繁的的split数据到disk, 由于磁盘IO有限, 就会产生上述错误.
- 解决[方案](https://support.imply.io/hc/en-us/articles/360009051253-Issue-Indexing-tasks-from-Kafka-or-Kinesis-are-finishing-successfully-but-without-any-data-ingested-):
  1. Increase taskCount
     - 数据量大的情况下, 增大task数, 分散IO压力, task数<= kafka partition数
  2. Increase maxRowsInMemory
     - 增大内存 buffer, 减少 split disk 次数
  3. Use a coarser segmentGranularity (e.g. change from HOUR to DAY)
     - 增大 segmentGranularity 可以减少 persists数量, 特别是stream 中 data 没有 time-ordered时.
## 3. Issue: Indexing tasks from Kafka or Kinesis are finishing successfully, but without any data ingested.
- 描述
    - index task finishing 并且任务状态失败(数据), 但是log中显示 task SUCCESS
        - ```text
            2018-07-02T07:46:02,783 ERROR [task-runner-0-priority-0] io.druid.indexing.overlord.ThreadPoolTaskRunner - Exception while running task[KinesisIndexTask{id=index_kinesis_xxxx, type=index_kinesis, dataSource=xxxxxx}]
            io.druid.java.util.common.ISE: Starting sequenceNumber [49585811274695675412070234775862022455403125352932835346] is no longer available for partition [shardId-000000000001] (earliest: [49585811274695675412070251479546550741235996701713498130]) 
                    and resetOffsetAutomatically is not enabled
          
          OR
          
            2018-09-03T12:55:14,522 WARN [task-runner-0-priority-0] io.druid.indexing.kafka.KafkaIndexTask - OffsetOutOfRangeException with message [Offsets out of range with no configured reset policy for partitions: {Druid-XXXX-X=7263946670}]
          ```
        - 之后日志中出现
        - ```text
          2018-09-03T12:55:15,207 INFO [publish-0] io.druid.segment.realtime.appenderator.BaseAppenderatorDriver - Nothing to publish, skipping publish step.
          
          .............
          
          2018-09-03T12:55:15,232 INFO [task-runner-0-priority-0] io.druid.indexing.overlord.TaskRunnerUtils - Task [index_kafka_Test-K_40675c261af88df_pcigmfli] status changed to [SUCCESS].
          ..... ......
          2018-09-03T12:55:15,233 INFO [task-runner-0-priority-0] io.druid.indexing.worker.executor.ExecutorLifecycle - Task completed with status: {
          "id" : "index_kafka_Test-K_40675c261af88df_pcigmfli",
          "status" : "SUCCESS",
          "duration" : 3601087,
          "errorMsg" : null
          }
          ```
- 原因
    - ingestion task 尝试读取kafka不可用的数据, 在 taskDuration 期间不断重试, 知道超时 或 supervisor reset.
    - 最常见的原因是, 由于kafka保留策略, 在kafka删除删除过期数据之前, ingestion 不能摄入所有消息.
    - 可能是 ingestion 缓慢, 需要扩容或优化, 也可能是 ingestion关闭时间太久, kafka已经消息过期.
- 解决方案: [原文链接](https://support.imply.io/hc/en-us/articles/360009051253-Issue-Indexing-tasks-from-Kafka-or-Kinesis-are-finishing-successfully-but-without-any-data-ingested-)
    - rest supervisor
    - 配置了 resetOffsetAutomatic , 但是kafka 只是暂时不可用.
    - useEarliestOffset: false